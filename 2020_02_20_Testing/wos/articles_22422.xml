<?xml version="1.0"?>
<records xmlns="http://scientific.thomsonreuters.com/schema/wok5.4/public/Fields"><REC r_id_disclaimer="ResearcherID data provided by Clarivate Analytics"><UID>WOS:000461898700004</UID><static_data><fullrecord_metadata><fund_ack><grants count="2"><grant><grant_agency>U.S. Army Corps of Engineers (USACE) Climate Preparedness and Resilience program</grant_agency></grant><grant><grant_agency>National Science Foundation</grant_agency></grant></grants></fund_ack><abstracts count="1"><abstract><abstract_text count="1"><p>This study presents a gridded meteorology intercomparison using the State of Hawaii as a testbed. This is motivated by the goal to provide the broad user community with knowledge of interproduct differences and the reasons differences exist. More generally, the challenge of generating station-based gridded meteorological surfaces and the difficulties in attributing interproduct differences to specific methodological decisions are demonstrated. Hawaii is a useful testbed because it is traditionally underserved, yet meteorologically interesting and complex. In addition, several climatological and daily gridded meteorology datasets are now available, which are used extensively by the applications modeling community, thus an intercomparison enhances Hawaiian specific capabilities. We compare PRISM climatology and three daily datasets: new datasets from the University of Hawai'i and the National Center for Atmospheric Research, and Daymet version 3 for precipitation and temperature variables only. General conclusions that have emerged are 1) differences in input station data significantly influence the product differences, 2) explicit prediction of precipitation occurrence is crucial across multiple metrics, and 3) attribution of differences to specific methodological choices is difficult and limits the usefulness of intercomparisons. Because generating gridded meteorological fields is an elaborate process with many methodological choices interacting in complex ways, future work should 1) develop modular frameworks that allows users to easily examine the breadth of methodological choices, 2) collate available nontraditional high-quality observational datasets for true out-of-sample validation and make them publicly available, and 3) define benchmarks of acceptable performance for methodological components and products.</p></abstract_text></abstract></abstracts></fullrecord_metadata></static_data><dynamic_data><cluster_related/></dynamic_data></REC></records>
