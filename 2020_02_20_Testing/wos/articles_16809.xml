<?xml version="1.0"?>
<records xmlns="http://scientific.thomsonreuters.com/schema/wok5.4/public/Fields"><REC r_id_disclaimer="ResearcherID data provided by Clarivate Analytics"><UID>WOS:000357833600045</UID><static_data><fullrecord_metadata><fund_ack><grants count="1"><grant><grant_agency>National Science Foundation (NSF)</grant_agency><grant_ids count="3"><grant_id>EAR-1215771</grant_id><grant_id>EAR-1215809</grant_id><grant_id>CBET-0729830</grant_id></grant_ids></grant></grants></fund_ack><abstracts count="1"><abstract><abstract_text count="1"><p>Everyone taking field observations has a story of data collection gone wrong, and in most cases, the errors in the data are immediately obvious. A more challenging problem occurs when the errors are insidious, i.e., not readily detectable, and the error-laden data appear useful for model testing and development. We present two case studies, one related to the water balance in the snow-fed Tuolumne River, Sierra Nevada, California, combined with modeling using the Distributed Hydrology Soil Vegetation Model (DHSVM); and one related to the energy balance at Snoqualmie Pass, Washington, combined with modeling using the Structure for Unifying Multiple Modeling Alternatives (SUMMA). In the Tuolumne, modeled streamflow in 1 year was more than twice as large as observed; at Snoqualmie, modeled nighttime surface temperatures were biased by about 110 degrees C. Both appeared to be modeling failures, until detective work uncovered observational errors. We conclude with a discussion of what these cases teach us about science in an age of specialized research, when one person collects data, a separate person conducts model simulations, and a computer is charged with data quality assurance.</p></abstract_text></abstract></abstracts></fullrecord_metadata></static_data><dynamic_data><cluster_related/></dynamic_data></REC></records>
