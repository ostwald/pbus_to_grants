<?xml version="1.0"?>
<records xmlns="http://scientific.thomsonreuters.com/schema/wok5.4/public/Fields"><REC r_id_disclaimer="ResearcherID data provided by Clarivate Analytics"><UID>WOS:000398147200022</UID><static_data><fullrecord_metadata><fund_ack><grants count="1"><grant><grant_agency>Short Term Explicit Prediction (STEP) program</grant_agency></grant></grants></fund_ack><abstracts count="1"><abstract><abstract_text count="1"><p>As high-resolution numerical weather prediction models are now commonplace, "neighborhood'' verification metrics are regularly employed to evaluate forecast quality. These neighborhood approaches relax the requirement that perfect forecasts must match observations at the grid scale, contrasting traditional point-by-point verification methods. One recently proposed metric, the neighborhood equitable threat score, is calculated from 2 x 2 contingency tables that are populated within a neighborhood framework. However, the literature suggests three subtly different methods of populating neighborhood-based contingency tables. Thus, this work compares and contrasts these three variants and shows they yield statistically significantly different conclusions regarding forecast performance, illustrating that neighborhood-based contingency tables should be constructed carefully and transparently. Furthermore, this paper shows how two of the methods use inconsistent event definitions and suggests a "neighborhood maximum'' approach be used to fill neighborhood-based contingency tables.</p></abstract_text></abstract></abstracts></fullrecord_metadata></static_data><dynamic_data><cluster_related/></dynamic_data></REC></records>
