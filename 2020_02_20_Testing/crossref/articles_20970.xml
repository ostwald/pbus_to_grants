<?xml version="1.0" encoding="UTF-8"?>
<doi_records>
  <doi_record owner="10.1145" timestamp="2017-06-25 00:45:49">
    <crossref>
      <conference>
        <contributors>
          <person_name sequence="first" contributor_role="chair">
            <given_name>Jack</given_name>
            <surname>Dongarra</surname>
          </person_name>
          <person_name sequence="additional" contributor_role="chair">
            <given_name>Daniel</given_name>
            <surname>Holmes</surname>
          </person_name>
          <person_name sequence="additional" contributor_role="chair">
            <given_name>Antonia</given_name>
            <surname>Collis</surname>
          </person_name>
          <person_name sequence="additional" contributor_role="chair">
            <given_name>Jesper</given_name>
            <surname>Larsson Tr√§ff</surname>
          </person_name>
          <person_name sequence="additional" contributor_role="chair">
            <given_name>Lorna</given_name>
            <surname>Smith</surname>
          </person_name>
        </contributors>
        <event_metadata>
          <conference_name>the 23rd European MPI Users' Group Meeting</conference_name>
          <conference_acronym>EuroMPI 2016</conference_acronym>
          <conference_sponsor>SIGHPC, ACM Special Interest Group on High Performance Computing, Special Interest Group on High Performance Computing</conference_sponsor>
          <conference_number>23</conference_number>
          <conference_location>Edinburgh, United Kingdom</conference_location>
          <conference_date start_month="09" start_year="2016" start_day="25" end_year="2016" end_month="09" end_day="28"/>
        </event_metadata>
        <proceedings_metadata language="en">
          <proceedings_title>Proceedings of the 23rd European MPI Users' Group Meeting on - EuroMPI 2016</proceedings_title>
          <publisher>
            <publisher_name>ACM Press</publisher_name>
            <publisher_place>New York, New York, USA</publisher_place>
          </publisher>
          <publication_date media_type="print">
            <year>2016</year>
          </publication_date>
          <isbn media_type="print">9781450342346</isbn>
          <doi_data>
            <doi>10.1145/2966884</doi>
            <timestamp>20161129112023</timestamp>
            <resource>http://dl.acm.org/citation.cfm?doid=2966884</resource>
          </doi_data>
        </proceedings_metadata>
        <conference_paper publication_type="full_text">
          <contributors>
            <person_name sequence="first" contributor_role="author">
              <given_name>Alessandro</given_name>
              <surname>Fanfarillo</surname>
              <affiliation>National Center for Atmospheric Research, Boulder, CO, USA</affiliation>
            </person_name>
            <person_name sequence="additional" contributor_role="author">
              <given_name>Jeff</given_name>
              <surname>Hammond</surname>
              <affiliation>Intel Corporation, Portland, OR, USA</affiliation>
            </person_name>
          </contributors>
          <titles>
            <title>CAF Events Implementation Using MPI-3 Capabilities</title>
          </titles>
          <publication_date media_type="print">
            <year>2016</year>
          </publication_date>
          <pages>
            <first_page>198</first_page>
            <last_page>207</last_page>
          </pages>
          <program name="AccessIndicators">
            <license_ref applies_to="vor" start_date="2016-09-25">http://www.acm.org/publications/policies/copyright_policy#Background</license_ref>
          </program>
          <doi_data>
            <doi>10.1145/2966884.2966916</doi>
            <timestamp>20161129112024</timestamp>
            <resource>http://dl.acm.org/citation.cfm?doid=2966884.2966916</resource>
            <collection property="crawler-based" setbyID="acm">
              <item crawler="iParadigms">
                <resource>http://dl.acm.org/ft_gateway.cfm?id=2966916&amp;ftid=1804896&amp;dwn=1</resource>
              </item>
            </collection>
          </doi_data>
          <citation_list>
            <citation key="key-10.1145/2966884.2966916-1">
              <unstructured_citation>Cray XC series network. Technical Report WP-Aries01-1112, Cray, 2012.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-2">
              <unstructured_citation>E. Allen, D. Chase, J. Hallett, V. Luchangco, J.-W. Maessen, S. Ryu, G. L. Steele, and S. Tobin-Hochstadt. The Fortress language specification. Technical report, Sun Microsystems, Inc., March 2008. Version 1.0.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-3">
              <doi provider="crossref">10.1109/HOTI.2010.23</doi>
              <unstructured_citation>R. Alverson, D. Roweth, and L. Kaplan. The Gemini system interconnect. High-Performance Interconnects, Symposium on, 0:83--87, 2010.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-4">
              <unstructured_citation>R. Belli and T. Hoefler. Notified access: Extending remote memory access programming models for Producer-Consumer synchronization. In IPDPS, Hyderabad, India, May 2015.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-5">
              <doi provider="crossref">10.1504/IJHPCN.2004.007569</doi>
              <unstructured_citation>D. Bonachea and J. Duell. Problems with using MPI 1.1 and 2.0 as compilation targets for parallel language implementations. International Journal of High Performance Computing and Networking, 1(1-3):91--99, 2004.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-6">
              <doi provider="crossref">10.1145/1006209.1006251</doi>
              <unstructured_citation>R. Brightwell and K. D. Underwood. An analysis of the impact of MPI overlap and independent progress. In Proceedings of the 18th Annual International Conference on Supercomputing, ICS '04, pages 298--305, New York, NY, USA, 2004. ACM.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-7">
              <unstructured_citation>V. Cardellini, A. Fanfarillo, and S. Filippone. Overlapping communication with computation in MPI applications. Technical Report DICII RR-16.09, Universita di Roma Tor Vergata, Feb. 2016. http://hdl.handle.net/2108/140530.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-8">
              <doi provider="crossref">10.1177/1094342007078442</doi>
              <unstructured_citation>B. Chamberlain, D. Callahan, and H. Zima. Parallel programmability and the Chapel language. Int. J. High Perform. Comput. Appl., 21(3):291--312, Aug. 2007.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-9">
              <doi provider="crossref">10.1145/1103845.1094852</doi>
              <unstructured_citation>P. Charles, C. Grothoff, V. Saraswat, C. Donawa, A. Kielstra, K. Ebcioglu, C. von Praun, and V. Sarkar. X10: An object-oriented approach to non-uniform cluster computing. SIGPLAN Not., 40(10):519--538, Oct. 2005.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-10">
              <doi provider="crossref">10.1145/2063384.2063419</doi>
              <unstructured_citation>D. Chen, N. A. Eisley, P. Heidelberger, R. M. Senger, Y. Sugawara, S. Kumar, V. Salapura, D. L. Satterfield, B. Steinmacher-Burow, and J. J. Parker. The IBM Blue Gene/Q interconnection network and message unit. In 2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC), pages 1--10, Nov 2011.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-11">
              <doi provider="crossref">10.1109/HiPC.2014.7116712</doi>
              <unstructured_citation>J. Daily, A. Vishnu, B. Palmer, H. van Dam, and D. Kerbyson. On the suitability of MPI as a PGAS runtime. In 21st International Conference on High Performance Computing (HiPC), 2014, pages 1--10, Dec 2014.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-12">
              <doi provider="crossref">10.1109/HPEC.2014.7040972</doi>
              <unstructured_citation>R. F. V. der Wijngaart and T. G. Mattson. The parallel research kernels. In High Performance Extreme Computing Conference (HPEC), 2014 IEEE, pages 1--6, Sept 2014.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-13">
              <doi provider="crossref">10.1109/IPDPS.2012.72</doi>
              <unstructured_citation>J. Dinan, P. Balaji, J. R. Hammond, S. Krishnamoorthy, and V. Tipparaju. Supporting the Global Arrays PGAS model using MPI one-sided communication. In IPDPS, pages 739--750, May 2012.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-14">
              <doi provider="crossref">10.1007/978-3-319-05215-1_12</doi>
              <unstructured_citation>J. Dinan, C. Cole, G. Jost, S. Smith, K. Underwood, and R. W. Wisniewski. Reducing synchronization overhead through bundled communication. In OpenSHMEM and Related Technologies. Experiences, Implementations, and Tools, pages 163--177. Springer, 2014.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-15">
              <doi provider="crossref">10.1145/1048935.1050169</doi>
              <unstructured_citation>T. H. Dunigan, Jr., M. R. Fahey, J. B. White III, and P. H. Worley. Early evaluation of the Cray X1. In Proceedings of the 2003 ACM/IEEE Conference on Supercomputing, SC '03, pages 18--, New York, NY, USA, 2003. ACM.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-16">
              <doi provider="crossref">10.1145/2676870.2676876</doi>
              <unstructured_citation>A. Fanfarillo, T. Burnus, V. Cardellini, S. Filippone, D. Nagle, and D. Rouson. OpenCoarrays: Open-source transport layers supporting coarray Fortran compilers. In PGAS, PGAS '14. ACM, Oct. 2014.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-17">
              <doi provider="crossref">10.1007/978-3-319-05215-1_4</doi>
              <unstructured_citation>J. R. Hammond, S. Ghosh, and B. M. Chapman. Implementing OpenSHMEM Using MPI-3 One-Sided Communication, pages 44--58. Springer International Publishing, Cham, 2014.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-18">
              <unstructured_citation>T. Hoefler, G. Bronevetsky, B. Barrett, B. R. D. Supinski, and A. Lumsdaine. Efficient MPI support for advanced hybrid programming models. In Recent Advances in the Message Passing Interface, volume 6305 of LNCS, pages 50--61. Springer, 2010.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-19">
              <doi provider="crossref">10.1109/CLUSTR.2008.4663774</doi>
              <unstructured_citation>T. Hoefler and A. Lumsdaine. Message progression in parallel computing - to thread or not to thread? In Proc. of 2008 IEEE Int'l Conf. on Cluster Computing, pages 213--222, Sept. 2008.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-20">
              <unstructured_citation>Intel. Distributed memory coarray Fortran with the Intel Fortran compiler for Linux: Essential guide, Nov. 2014.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-21">
              <unstructured_citation>ISO/IEC/JTC1/SC22/WG5. TS 18508 additional parallel features in Fortran, Aug. 2015.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-22">
              <unstructured_citation>R. E. Kessler and J. L. Schwarzmeier. Cray T3D: a new dimension for Cray Research. In Compcon Spring '93, Digest of Papers., pages 176--182, Feb 1993.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-23">
              <unstructured_citation>M. Metcalf, J. Reid, and M. Cohen. Modern Fortran Explained. Oxford University Press, Inc., New York, NY, USA, 4th edition, 2011.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-24">
              <doi provider="crossref">10.1007/BF00130708</doi>
              <unstructured_citation>J. Nieplocha, R. J. Harrison, and R. J. Littlefield. Global arrays: A nonuniform memory access programming model for high-performance computers. The Journal of Supercomputing, 10(2):169--189, 1996.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-25">
              <doi provider="crossref">10.1145/289918.289920</doi>
              <unstructured_citation>R. Numrich and J. Reid. Co-array Fortran for parallel programming. SIGPLAN Fortran Forum, 17(2):1--31, Aug. 1998.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-26">
              <doi provider="crossref">10.1145/1080399.1080400</doi>
              <unstructured_citation>R. W. Numrich and J. Reid. Co-arrays in the next Fortran standard. SIGPLAN Fortran Forum, 24(2):4--17, Aug. 2005.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-27">
              <doi provider="crossref">10.1109/40.988689</doi>
              <unstructured_citation>F. Petrini, W.-c. Feng, A. Hoisie, S. Coll, and E. Frachtenberg. The Quadrics network: High-performance clustering technology. IEEE Micro, 22(1):46--57, Jan. 2002.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-28">
              <unstructured_citation>S. L. Scott and et al. The Cray T3E network: Adaptive routing in a high performance 3D torus, 1996.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-29">
              <unstructured_citation>M. Si, A. Pena, J. Hammond, P. Balaji, M. Takagi, and Y. Ishikawa. Casper: An asynchronous progress model for MPI RMA on many-core architectures. IPDPS '15, pages 665--676, May 2015.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-30">
              <unstructured_citation>UPC Consortium. UPC language specifications, v1.2. Tech Report LBNL-59208, Lawrence Berkeley National Lab, 2005.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-31">
              <doi provider="crossref">10.1007/978-3-319-41321-1_17</doi>
              <unstructured_citation>R. F. Van der Wijngaart, A. Kayi, J. R. Hammond, G. Jost, T. St. John, S. Sridharan, T. G. Mattson, J. Abercrombie, and J. Nelson. Comparing runtime systems with exascale ambitions using the Parallel Research Kernels. In ISC High Performance, pages 321--339, 2016.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-32">
              <unstructured_citation>R. F. Van der Wijngaart, S. Sridharan, A. Kayi, G. Jost, J. R. Hammond, T. G. Mattson, and J. E. Nelson. Using the Parallel Research Kernels to study PGAS models. In PGAS. IEEE, 2015.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-33">
              <doi provider="crossref">10.1109/HiPC.2012.6507506</doi>
              <unstructured_citation>A. Vishnu, J. Daily, and B. Palmer. Designing scalable PGAS communication subsystems on Cray Gemini interconnect. In 19th International Conference on High Performance Computing (HiPC), 2012, pages 1--10, Dec 2012.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-34">
              <doi provider="crossref">10.1109/IPDPSW.2013.262</doi>
              <unstructured_citation>A. Vishnu, D. Kerbyson, K. Barker, and H. van Dam. Building scalable PGAS communication subsystem on Blue Gene/Q. In IPDPSW, pages 825--833, May 2013.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-35">
              <doi provider="crossref">10.1109/CCGRID.2010.58</doi>
              <unstructured_citation>A. Vishnu and M. Krishnan. Efficient on-demand connection management mechanisms with PGAS models over InfiniBand. In CCGrid, pages 175--184, May 2010.</unstructured_citation>
            </citation>
            <citation key="key-10.1145/2966884.2966916-36">
              <unstructured_citation>H. Zhou, K. Idrees, and J. Gracia. Leveraging MPI-3 shared-memory extensions for efficient PGAS runtime systems. In Euro-Par, pages 373--384, 2015.</unstructured_citation>
            </citation>
          </citation_list>
        </conference_paper>
      </conference>
    </crossref>
  </doi_record>
</doi_records>
