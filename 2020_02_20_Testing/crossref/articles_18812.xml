<?xml version="1.0" encoding="UTF-8"?>
<doi_records>
  <doi_record owner="10.5194" timestamp="2018-08-15 11:36:06">
    <crossref>
      <journal>
        <journal_metadata language="en" metadata_distribution_opts="any" reference_distribution_opts="any">
          <full_title>Geoscientific Model Development</full_title>
          <abbrev_title>Geosci. Model Dev.</abbrev_title>
          <issn media_type="electronic">1991-9603</issn>
        </journal_metadata>
        <journal_issue>
          <publication_date media_type="online">
            <year>2016</year>
          </publication_date>
          <journal_volume>
            <volume>9</volume>
          </journal_volume>
          <issue>8</issue>
        </journal_issue>
        <journal_article publication_type="full_text">
          <titles>
            <title>Asynchronous communication in spectral-element and discontinuous Galerkin methods for atmospheric dynamics
– 
a case study using the High-Order Methods Modeling Environment (HOMME-homme_dg_branch)</title>
          </titles>
          <contributors>
            <person_name sequence="first" contributor_role="author">
              <given_name>Benjamin F.</given_name>
              <surname>Jamroz</surname>
            </person_name>
            <person_name sequence="additional" contributor_role="author">
              <given_name>Robert</given_name>
              <surname>Klöfkorn</surname>
            </person_name>
          </contributors>
          <abstract>
            <p><![CDATA[<p><strong>Abstract.</strong> The scalability of computational applications on current and next-generation supercomputers is increasingly limited by the cost of inter-process communication. We implement non-blocking asynchronous communication in the High-Order Methods Modeling Environment for the time integration of the hydrostatic fluid equations using both the spectral-element and discontinuous Galerkin methods. This allows the overlap of computation with communication, effectively hiding some of the costs of communication. A novel detail about our approach is that it provides some data movement to be performed during the asynchronous communication even in the absence of other computations. This method produces significant performance and scalability gains in large-scale simulations.</p>]]></p>
          </abstract>
          <publication_date media_type="online">
            <month>08</month>
            <day>26</day>
            <year>2016</year>
          </publication_date>
          <pages>
            <first_page>2881</first_page>
            <last_page>2892</last_page>
          </pages>
          <program name="AccessIndicators">
            <license_ref>https://creativecommons.org/licenses/by/3.0/</license_ref>
          </program>
          <doi_data>
            <doi>10.5194/gmd-9-2881-2016</doi>
            <resource>https://www.geosci-model-dev.net/9/2881/2016/</resource>
            <collection property="crawler-based">
              <item crawler="iParadigms">
                <resource>https://www.geosci-model-dev.net/9/2881/2016/gmd-9-2881-2016.pdf</resource>
              </item>
            </collection>
          </doi_data>
          <citation_list>
            <citation key="ref1">
              <unstructured_citation>The 2012 Dynamical Core Model Intercomparison Project: available at: https://earthsystemcog.org/projects/dcmip-2012 (last access: 22 August 2016), 2012.</unstructured_citation>
            </citation>
            <citation key="ref2">
              <unstructured_citation>Baggag, A., Atkins, H., and Keyes, D.: Parallel Implementation of the Discontinuous Galerkin Method, in: Proceedings of Parallel CFD'99, 115–122, 1999.</unstructured_citation>
            </citation>
            <citation key="ref3">
              <doi provider="crossref">10.5194/gmd-8-2829-2015</doi>
              <unstructured_citation>Baker, A. H., Hammerling, D. M., Levy, M. N., Xu, H., Dennis, J. M., Eaton, B. E., Edwards, J., Hannay, C., Mickelson, S. A., Neale, R. B., Nychka, D., Shollenberger, J., Tribbia, J., Vertenstein, M., and Williamson, D.: A new ensemble-based consistency test for the Community Earth System Model (pyCECT v1.0), Geosci. Model Dev., 8, 2829–2840, https://doi.org/10.5194/gmd-8-2829-2015, 2015.</unstructured_citation>
            </citation>
            <citation key="ref4">
              <doi provider="crossref">10.1145/2503210.2503265</doi>
              <unstructured_citation>Bermejo-Moreno, I., Bodart, J., Larsson, J., Barney, B., Nichols, J., and Jones, S.: Solving the compressible Navier-Stokes equations on up to 1.97 million cores and 4.1 trillion grid points, in: Proceedings of SC13: International Conference for High Performance Computing, Networking, Storage and Analysis, Denver, 62:1–62:10, 2013.</unstructured_citation>
            </citation>
            <citation key="ref5">
              <unstructured_citation>Brömmel, D., Frings, W., and Wylie, B. J. N.: JUQUEEN Extreme Scaling Workshop 2015, Tech. Rep. FZJ-JSC-IB-2015-01, available at: http://juser.fz-juelich.de/record/188191 (last access: 22 August 2016), 2015.</unstructured_citation>
            </citation>
            <citation key="ref6">
              <unstructured_citation>Brömmel, D., Frings, W., and Wylie, B. J. N.: JUQUEEN Extreme Scaling Workshop 2016, Tech. Rep. FZJ-JSC-IB-2016-01, available at: https://juser.fz-juelich.de/record/283461, last access: 22 August 2016.</unstructured_citation>
            </citation>
            <citation key="ref7">
              <unstructured_citation>Chhugani, J., Kim, C., Shukla, H., Park, J., Dubey, P., Shalf, J., and Simon, H. D.: Billion-particle SIMD-friendly Two-point Correlation on Large-scale HPC Cluster Systems, in: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC '12, IEEE Computer Society Press, Los Alamitos, CA, USA, 1:1–1:11, http://dl.acm.org/citation.cfm?id=2388996.2388998, 2012.</unstructured_citation>
            </citation>
            <citation key="ref8">
              <unstructured_citation>Computational and Information Systems Laboratory. Yellowstone: IBM iDataPlex System (Climate Simulation Laboratory), Boulder, CO: National Center for Atmospheric Research, available at: http://n2t.net/ark:/85065/d7wd3xhc (last access: 22 August 2016), 2012.</unstructured_citation>
            </citation>
            <citation key="ref9">
              <doi provider="crossref">10.1177/1094342011428142</doi>
              <unstructured_citation>Dennis, J. M., Edwards, J., Evans, K. J., Guba, O., Lauritzen, P. H., Mirin, A. A., St.-Cyr, A., Taylor, M. A., and Worley, P. H.: CAM-SE: A scalable spectral element dynamical core for the Community Atmosphere Model, IJHPCA, 26, 74–89, https://doi.org/10.1177/1094342011428142, 2012.</unstructured_citation>
            </citation>
            <citation key="ref10">
              <doi provider="crossref">10.1016/j.jcp.2013.08.050</doi>
              <unstructured_citation>Erath, C. and Nair, R.: A conservative multi-tracer transport scheme for spectral-element spherical grids, J. Comput. Phys., 256, 118–134, https://doi.org/10.1016/j.jcp.2013.08.050, 2014.</unstructured_citation>
            </citation>
            <citation key="ref11">
              <doi provider="crossref">10.1016/j.procs.2012.04.106</doi>
              <unstructured_citation>Erath, C., Lauritzen, P. H., Garcia, J. H., and Tufo, H. M.: Proceedings of the International Conference on Computational Science, ICCS 2012 Integrating a scalable and effcient semi-Lagrangian multi-tracer transport scheme in HOMME, Procedia Computer Science, 9, 994–1003, https://doi.org/10.1016/j.procs.2012.04.106, 2012.</unstructured_citation>
            </citation>
            <citation key="ref12">
              <unstructured_citation>Forum, M. P.: MPI: A Message-Passing Interface Standard, Tech. rep., Knoxville, TN, USA, 1994.</unstructured_citation>
            </citation>
            <citation key="ref13">
              <doi provider="crossref">10.1175/2011JCLI4083.1</doi>
              <unstructured_citation>Gent, P. R., Danabasoglu, G., Donner, L. J., Holland, M. M., Hunke, E. C., Jayne, S. R., Lawrence, D. M., Neale, R. B., Rasch, P. J., Vertenstein, M., Worley, P. H., Yang, Z.-L., and Zhang, M.: The Community Climate System Model Version 4, J. Climate, 24, 4973–4991, https://doi.org/10.1175/2011JCLI4083.1, 2011.</unstructured_citation>
            </citation>
            <citation key="ref14">
              <doi provider="crossref">10.1109/SC.2014.6</doi>
              <unstructured_citation>Heinecke, A., Breuer, A., Rettenberger, S., Bader, M., Gabriel, A. A., Pelties, C., Bode, A., Barth, W., Liao, X. K., Vaidyanathan, K., Smelyanskiy, M., and Dubey, P.: Petascale High Order Dynamic Rupture Earthquake Simulations on Heterogeneous Supercomputers, in: SC14: International Conference for High Performance Computing, Networking, Storage and Analysis, 3–14, https://doi.org/10.1109/SC.2014.6, 2014.</unstructured_citation>
            </citation>
            <citation key="ref15">
              <doi provider="crossref">10.1256/qj.06.12</doi>
              <unstructured_citation>Jablonowski, C. and Williamson, D. L.: A baroclinic instability test case for atmospheric model dynamical cores, Q. J. Roy. Meteor. Soc., 132, 2943–2975, https://doi.org/10.1256/qj.06.12, 2006.</unstructured_citation>
            </citation>
            <citation key="ref16">
              <doi provider="crossref">10.1145/363707.363723</doi>
              <unstructured_citation>Kahan, W.: Pracniques: Further Remarks on Reducing Truncation Errors, Commun. ACM, 8, 40, https://doi.org/10.1145/363707.363723, 1965.</unstructured_citation>
            </citation>
            <citation key="ref17">
              <doi provider="crossref">10.1016/j.crme.2010.11.002</doi>
              <unstructured_citation>Keyes, D.: Exaflop/s: The why and the how, Comptes Rendus Mécanique, 339, 70–77, https://doi.org/10.1016/j.crme.2010.11.002, 2011.</unstructured_citation>
            </citation>
            <citation key="ref18">
              <doi provider="crossref">10.1016/j.parco.2014.06.002</doi>
              <unstructured_citation>Kodama, C., Terai, M., Noda, A. T., Yamada, Y., Satoh, M., Seiki, T., ichi Iga, S., Yashiro, H., Tomita, H., and Minami, K.: Scalable rank-mapping algorithm for an icosahedral grid sy stem on the massive parallel computer with a 3-D torus network , Parallel Comput., 40, 362–373, https://doi.org/10.1016/j.parco.2014.06.002, 2014.</unstructured_citation>
            </citation>
            <citation key="ref19">
              <doi provider="crossref">10.1029/2010JA015586</doi>
              <unstructured_citation>Liu, H.-L., Foster, B. T., Hagan, M. E., McInerney, J. M., Maute, A., Qian, L., Richmond, A. D., Roble, R. G., Solomon, S. C., Garcia, R. R., Kinnison, D., Marsh, D. R., Smith, A. K., Richter, J., Sassi, F., and Oberheide, J.: Thermosphere extension of the Whole Atmosphere Community Climate Model, J. Geophys. Res.-Space, 115, A12302, https://doi.org/10.1029/2010JA015586, 2010.</unstructured_citation>
            </citation>
            <citation key="ref20">
              <unstructured_citation>Müller, A., Kopera, M. A., Marras, S., Wilcox, L. C., Isaac, T., and Giraldo, F. X.: Strong Scaling for Numerical Weather Prediction at Petascale with the Atmospheric Model NUMA, CoRR, abs/1511.01561, http://arxiv.org/abs/1511.01561 (last access: 22 August 2016), 2015.</unstructured_citation>
            </citation>
            <citation key="ref21">
              <doi provider="crossref">10.1016/j.compfluid.2008.04.006</doi>
              <unstructured_citation>Nair, R., Choi, H.-W., and Tufo, H.: Computational aspects of a scalable high-order discontinuous Galerkin atmospheric dynamical core, Comput. Fluids, 38, 309–319, https://doi.org/10.1016/j.compfluid.2008.04.006, 2009.</unstructured_citation>
            </citation>
            <citation key="ref22">
              <unstructured_citation>Nair, R. D.: Diffusion Experiments with a Global Discontinuous Galerkin Shallow Water Model, Mon. Weather Rev., 137, 3339–3350, 2009.</unstructured_citation>
            </citation>
            <citation key="ref23">
              <doi provider="crossref">10.2514/6.2016-3888</doi>
              <unstructured_citation>Nair, R. D., Bao, L., Toy, M. D., and Klöfkorn, R.: A High-Order Multiscale Global Atmospheric Model, in: 8th AIAA Atmospheric and Space Environments Conference, AIAA Aviation, (AIAA 2016-3888), https://doi.org/10.2514/6.2016-3888, 2016.</unstructured_citation>
            </citation>
            <citation key="ref24">
              <unstructured_citation>Neale, R. B., Gettelman, A., Park, S., Conley, A. J., Kinnison, D., Marsh, D., Smith, A. K., Vitt, F., Morrison, H., Cameron-Smith, P., Collins, W. D., Iacono, M. J., Easter, R. C., Liu, X., Taylor, M. A., Chen, C.-C., Lauritzen, P. H., Williamson, D. L., Garcia, R., Lamarque, J.-F., Mills, M., Tilmes, S., Ghan, S. J., and Rasch, P. J.: Description of the NCAR Community Atmosphere Model (CAM 5.0), NCAR Tech. Note, p. 268, available at: http://www.cesm.ucar.edu/models/cesm1.0/cam/ (last access: 22 August 2016), 2010.</unstructured_citation>
            </citation>
            <citation key="ref25">
              <doi provider="crossref">10.1145/2807591.2807675</doi>
              <unstructured_citation>Rudi, J., Malossi, A. C. I., Isaac, T., Stadler, G., Gurnis, M., Staar, P. W. J., Ineichen, Y., Bekas, C., Curioni, A., and Ghattas, O.: An Extreme-scale Implicit Solver for Complex PDEs: Highly Heterogeneous Flow in Earth's Mantle, in: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '15, ACM, New York, NY, USA, 5:1–5:12, https://doi.org/10.1145/2807591.2807675, 2015.</unstructured_citation>
            </citation>
            <citation key="ref26">
              <unstructured_citation>Sandia MPI Micro-Benchmark Suite (SMB): available at: http://www.cs.sandia.gov/smb/, last access: 22 August 2016.</unstructured_citation>
            </citation>
            <citation key="ref27">
              <unstructured_citation>Simmons, A. J. and Burridge, D. M.: An Energy and Angular-Momentum Conserving Vertical Finite-Difference Scheme and Hybrid Vertical Coordinates, Mon. Weather Rev., 109, 758–766, https://doi.org/10.1175/1520-0493(1981)109&amp;amp;lt;0758:AEAAMC&amp;amp;gt;2.0.CO;2, 1981.</unstructured_citation>
            </citation>
            <citation key="ref28">
              <doi provider="crossref">10.1002/2014MS000363</doi>
              <unstructured_citation>Small, R. J., Bacmeister, J., Bailey, D., Baker, A., Bishop, S., Bryan, F., Caron, J., Dennis, J., Gent, P., ming Hsu, H., Jochum, M., Lawrence, D., Munoz, E., diNezio, P., Sheitlin, T., Tomas, R., Tribbia, J., Heng Tseng, Y., and Vertenstein, M.: A new synoptic scale resolving global climate simulation using the Community Earth System Model, J. Adv. Model. Earth Syst., 6, 1065–1094, https://doi.org/10.1002/2014MS000363, 2014.</unstructured_citation>
            </citation>
            <citation key="ref29">
              <doi provider="crossref">10.1016/j.jcp.2010.04.008</doi>
              <unstructured_citation>Taylor, M. A. and Fournier, A.: A Compatible and Conservative Spectral Element Method on Unstructured Grids, J. Comput. Phys., 229, 5879–5895, https://doi.org/10.1016/j.jcp.2010.04.008, 2010.</unstructured_citation>
            </citation>
            <citation key="ref30">
              <unstructured_citation>Wittmann, M., Hager, G., Zeiser, T., and Wellein, G.: Asynchronous MPI for the Masses, CoRR, abs/1302.4280, available at: http://arxiv.org/abs/1302.4280 (last access: 22 August 2016), 2013.</unstructured_citation>
            </citation>
          </citation_list>
        </journal_article>
      </journal>
    </crossref>
  </doi_record>
</doi_records>
